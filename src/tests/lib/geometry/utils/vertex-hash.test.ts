/**
 * Tests for Fast Vertex Hashing for N-dimensional Deduplication
 *
 * @see src/lib/geometry/utils/vertex-hash.ts
 */

import { describe, it, expect } from 'vitest'
import {
  hashVertex,
  verticesEqual,
  vertexToKey,
  VertexHashSet,
  deduplicateVertices,
} from '@/lib/geometry/utils/vertex-hash'
import type { VectorND } from '@/lib/math'

describe('hashVertex', () => {
  it('should return consistent hash for same vertex', () => {
    const vertex: VectorND = [1.0, 2.0, 3.0]
    const hash1 = hashVertex(vertex)
    const hash2 = hashVertex(vertex)
    expect(hash1).toBe(hash2)
  })

  it('should return different hash for different vertices', () => {
    const v1: VectorND = [1, 2, 3]
    const v2: VectorND = [3, 2, 1]
    expect(hashVertex(v1)).not.toBe(hashVertex(v2))
  })

  it('should return 32-bit unsigned integer', () => {
    const hash = hashVertex([1, 2, 3, 4, 5])
    expect(hash).toBeGreaterThanOrEqual(0)
    expect(hash).toBeLessThanOrEqual(0xffffffff)
  })

  it('should handle empty vertex', () => {
    const hash = hashVertex([])
    expect(typeof hash).toBe('number')
    expect(hash).toBeGreaterThanOrEqual(0)
  })

  it('should handle high-dimensional vertices', () => {
    const vertex: VectorND = Array(11).fill(0).map((_, i) => i)
    const hash = hashVertex(vertex)
    expect(typeof hash).toBe('number')
    expect(hash).toBeGreaterThanOrEqual(0)
  })

  it('should handle negative coordinates', () => {
    const v1: VectorND = [1, 2, 3]
    const v2: VectorND = [-1, -2, -3]
    expect(hashVertex(v1)).not.toBe(hashVertex(v2))
  })

  it('should quantize coordinates within tolerance to same hash', () => {
    // Coordinates within 1e-6 tolerance should hash the same
    const v1: VectorND = [1.0000001, 2.0, 3.0]
    const v2: VectorND = [1.0000002, 2.0, 3.0]
    // After quantization (multiply by 1e6 and round), these should be equal
    expect(hashVertex(v1)).toBe(hashVertex(v2))
  })
})

describe('verticesEqual', () => {
  it('should return true for identical vertices', () => {
    const v: VectorND = [1, 2, 3]
    expect(verticesEqual(v, v)).toBe(true)
  })

  it('should return true for vertices within tolerance', () => {
    const v1: VectorND = [1.0, 2.0, 3.0]
    const v2: VectorND = [1.0000005, 2.0000005, 3.0000005]
    expect(verticesEqual(v1, v2)).toBe(true)
  })

  it('should return false for vertices beyond tolerance', () => {
    const v1: VectorND = [1.0, 2.0, 3.0]
    const v2: VectorND = [1.01, 2.0, 3.0]
    expect(verticesEqual(v1, v2)).toBe(false)
  })

  it('should return false for different dimensions', () => {
    const v1: VectorND = [1, 2, 3]
    const v2: VectorND = [1, 2]
    expect(verticesEqual(v1, v2)).toBe(false)
  })

  it('should handle empty vertices', () => {
    expect(verticesEqual([], [])).toBe(true)
  })

  it('should handle boundary case at exact tolerance', () => {
    const tolerance = 1e-6
    const v1: VectorND = [1.0, 2.0, 3.0]
    const v2: VectorND = [1.0 + tolerance * 0.9, 2.0, 3.0]
    expect(verticesEqual(v1, v2)).toBe(true)

    const v3: VectorND = [1.0 + tolerance * 1.1, 2.0, 3.0]
    expect(verticesEqual(v1, v3)).toBe(false)
  })
})

describe('vertexToKey', () => {
  it('should create string key with 6 decimal places', () => {
    const vertex: VectorND = [1.123456789, 2.0, 3.0]
    const key = vertexToKey(vertex)
    expect(key).toBe('1.123457,2.000000,3.000000')
  })

  it('should handle negative coordinates', () => {
    const key = vertexToKey([-1, -2, -3])
    expect(key).toBe('-1.000000,-2.000000,-3.000000')
  })

  it('should handle empty vertex', () => {
    expect(vertexToKey([])).toBe('')
  })
})

describe('VertexHashSet', () => {
  it('should add unique vertices', () => {
    const set = new VertexHashSet()
    expect(set.add([0, 0, 0])).toBe(true)
    expect(set.add([1, 0, 0])).toBe(true)
    expect(set.size).toBe(2)
  })

  it('should reject duplicate vertices', () => {
    const set = new VertexHashSet()
    set.add([1, 2, 3])
    expect(set.add([1, 2, 3])).toBe(false)
    expect(set.size).toBe(1)
  })

  it('should reject vertices within tolerance when hash matches', () => {
    const set = new VertexHashSet()
    set.add([1.0, 2.0, 3.0])
    // Same quantized values will be rejected
    expect(set.add([1.0, 2.0, 3.0])).toBe(false)
    expect(set.size).toBe(1)

    // Vertices with same hash but checked via verticesEqual
    // Note: Due to quantization boundaries, 1.0000005 may hash differently
    // than 1.0 even though they're within tolerance. The hash is an
    // optimization that catches exact matches; collision resolution
    // handles the rest.
  })

  it('should use has() to check existence', () => {
    const set = new VertexHashSet()
    set.add([1, 2, 3])
    expect(set.has([1, 2, 3])).toBe(true)
    expect(set.has([4, 5, 6])).toBe(false)
  })

  it('should clear all vertices', () => {
    const set = new VertexHashSet()
    set.add([1, 2, 3])
    set.add([4, 5, 6])
    set.clear()
    expect(set.size).toBe(0)
    expect(set.has([1, 2, 3])).toBe(false)
  })

  it('should handle hash collisions correctly', () => {
    const set = new VertexHashSet()
    // Add many vertices - some may have hash collisions
    for (let i = 0; i < 100; i++) {
      set.add([i, i * 2, i * 3])
    }
    expect(set.size).toBe(100)

    // All should be found
    for (let i = 0; i < 100; i++) {
      expect(set.has([i, i * 2, i * 3])).toBe(true)
    }
  })
})

describe('deduplicateVertices', () => {
  it('should remove duplicate vertices', () => {
    const vertices: VectorND[] = [
      [0, 0, 0],
      [1, 0, 0],
      [0, 0, 0], // duplicate of 0
      [1, 0, 0], // duplicate of 1
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)
    expect(unique.length).toBe(2)
    expect(indexMap).toEqual([0, 1, 0, 1])
  })

  it('should preserve order of first occurrence', () => {
    const vertices: VectorND[] = [
      [2, 0, 0],
      [1, 0, 0],
      [2, 0, 0], // duplicate
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)
    expect(unique[0]).toEqual([2, 0, 0])
    expect(unique[1]).toEqual([1, 0, 0])
    expect(indexMap).toEqual([0, 1, 0])
  })

  it('should handle all unique vertices', () => {
    const vertices: VectorND[] = [
      [0, 0, 0],
      [1, 0, 0],
      [2, 0, 0],
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)
    expect(unique.length).toBe(3)
    expect(indexMap).toEqual([0, 1, 2])
  })

  it('should handle all duplicate vertices', () => {
    const vertices: VectorND[] = [
      [1, 2, 3],
      [1, 2, 3],
      [1, 2, 3],
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)
    expect(unique.length).toBe(1)
    expect(indexMap).toEqual([0, 0, 0])
  })

  it('should handle empty array', () => {
    const { unique, indexMap } = deduplicateVertices([])
    expect(unique.length).toBe(0)
    expect(indexMap.length).toBe(0)
  })

  it('should deduplicate exact duplicates', () => {
    // Note: Hash-based deduplication uses quantization (multiply by 1e6, round)
    // Vertices may have same hash but still pass through if on quantization boundary.
    // For guaranteed deduplication, use exact same coordinates.
    const vertices: VectorND[] = [
      [1.0, 2.0, 3.0],
      [1.0, 2.0, 3.0], // exact duplicate
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)
    expect(unique.length).toBe(1)
    expect(indexMap).toEqual([0, 0])
  })

  it('should handle large arrays efficiently', () => {
    // Create 1000 vertices with 50% duplicates
    const vertices: VectorND[] = []
    for (let i = 0; i < 500; i++) {
      vertices.push([i, i * 2, i * 3])
      vertices.push([i, i * 2, i * 3]) // duplicate
    }

    const start = performance.now()
    const { unique, indexMap } = deduplicateVertices(vertices)
    const elapsed = performance.now() - start

    expect(unique.length).toBe(500)
    expect(indexMap.length).toBe(1000)
    // Should complete quickly (not O(n^2))
    expect(elapsed).toBeLessThan(100) // 100ms should be plenty
  })

  it('should correctly map indices for edge updates', () => {
    // Simulate edge index remapping use case
    const vertices: VectorND[] = [
      [0, 0, 0], // index 0
      [1, 0, 0], // index 1
      [0, 0, 0], // index 2 - duplicate of 0
      [2, 0, 0], // index 3
    ]

    const { unique, indexMap } = deduplicateVertices(vertices)

    // Original edge [1, 2] should become [1, 0] after dedup
    const oldEdge = [1, 2]
    const newEdge = [indexMap[oldEdge[0]!]!, indexMap[oldEdge[1]!]!]
    expect(newEdge).toEqual([1, 0])

    expect(unique.length).toBe(3)
  })
})
